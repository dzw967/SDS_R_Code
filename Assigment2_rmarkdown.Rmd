---
title: "Assignment 2 - Social Data Science"
author: "Adam Ingwersen"
date: "3. nov. 2015"
output: html_document
---

# **Scrape & Analysis of www.ipaidabribe.com**

We are to scrape the website and clean the gathered data primarily using the R-packages: *stringr*, *plyr*, *dplyr* & *rvest*. Then, we will delve into an econometric analysis paired with geo-spatial data-analysis using *ggplot*, *countrycode* etc.

## **Scraping the [website](http://ipaidabribe.com/reports) : Steps 1-2**

The website is constructed in a way such that for each page of recorded bribes contains 10 posts per page. It did not prove useful to use selector-gadget for identifying the underlying html-table. However, any page of 10 posts only has unique URL-element; http://www.ipaidabribe.com/reports/paid?page={**any integer**}. Thus, we took the following approach: create vector of integers from 0:1000 in intervals of 10. Insert 0, 10, 20 etc. into the element in URL that determines page number via loop - here looping with plyr was deemed useful as it is simple to coerce into dataframe and is rather fast. 

```{r create initial links, echo=FALSE, warning = FALSE, include = FALSE}
library("knitr")
knitr::opts_chunk$set(cache=TRUE)

library("plyr")
library("rvest")
library("stringr") 


# 1.1) 
#Need to create values of 0:1000 by 10 in order construct list of viable URLS to scrape for the appropriate css.selector for each post.
x1000 <- c(0:1000)
n1000 <- length(x1000)
var1000 = x1000[seq(1, n1000, 10)]

#Looking at the structure of the URL's we see that /reports/paid?page= defines the span for subsets of 10 posts
# Thus, if we can insert 0:1000 by intervals of 10 into "LANK", we actually have all 100 pages, each consisting of 10 posts 
linksub.li = "http://www.ipaidabribe.com/reports/paid?page=LANK"


# 1.2) FUNCTION : 
#Create function that runs through all numbers in var1000 and replaces LANK with the 0:1000 by 10s

link_str_replace = function(var1000){
  link.sub = gsub("\\LANK", var1000, linksub.li)
}

# 1.3) LOOP : 
# Using plyr (ld) for simple, efficient looping - list-to-list, llply works, but not optimally as transformations has to be made afterwards
num.link.li = ldply(var1000, link_str_replace)
num.link.li2 = num.link.li$V1

```
```{r, echo = TRUE}
head(num.link.li2, 5)

```

We have now obtained 100 pages each containing 10 posts. Directly from these pages, the post-information we are interested in is accessible, given the appropriate CSS-selectors:

```{r css-selectors, echo = TRUE}
## Let's identify the relevant css-selectors

as2.css.selector_1 = ".heading-3 a"       #URL and/or TITLE depends on 'html_attr(name = href/title)'
as2.css.selector_2 = "span.date"          #date
as2.css.selector_3 = "li.paid-amount"     #amount paid
as2.css.selector_4 = "div.key > a"        #location
as2.css.selector_5 = "li.name > a"        #department
as2.css.selector_6 = "li.transaction > a" #transaction details
as2.css.selector_7 = "li.views"           #number of views

```

Defining function and creating loop for iterating trhough all observations in **num.link.li2**. We make use of **rvest** in order to fetch data. 

```{r, function and loop for scrape, echo = FALSE, warning = FALSE, include = FALSE}
# Function requesting on CSS-selectors for each webpage

scrape_post_bribe = function(num.link.li2){
  post.url = read_html(num.link.li2, encoding = "UTF-8")
  post.title = post.url %>%
    html_nodes(css = as2.css.selector_1) %>%
    html_attr(name = 'title')
  post.date = post.url %>%
    html_nodes(css = as2.css.selector_2) %>%
    html_text()
  post.paid = post.url %>%
    html_nodes(css = as2.css.selector_3) %>%
    html_text()
  post.location = post.url %>%
    html_nodes(css = as2.css.selector_4) %>%
    html_attr(name = 'title')
  post.dept = post.url %>%
    html_nodes(css = as2.css.selector_5) %>%
    html_attr(name = 'title')
  post.trans = post.url %>%
    html_nodes(css = as2.css.selector_6) %>%
    html_attr(name = 'title')
  post.views = post.url %>%
    html_nodes(css = as2.css.selector_7) %>%
    html_text()
  return(cbind(post.title, post.date, post.location, post.dept, post.trans, post.views, post.url))
}

# Loop - sleep-timer set to 0.01 

post.bribe.df = list()
for(i in num.link.li2){
  print(paste("processing", i, sep = " "))
  post.bribe.df[[i]] = scrape_post_bribe(i)
  Sys.sleep(0.01)
  cat("done!\n")
}

```

When the 1.000 posts have been obtained, we coerce from list into a dataframe using *ldply([data], data.frame)*. From here we need to do basic data-cleaning primarily utilizing functions contained in *stringr* in order to have a tidy, interpretable dataframe. 

```{r, echo = FALSE}
#DATA :
# Now that the data has been gathered, we need to do a little cleaning - first step is to set up a dataframe and remove duplicate observations
# ldply(data, data.frame) fixes this for us
IN.Bribe.df = ldply(post.bribe.df, data.frame)
# We now have a dataframe of 4012 observations - this is due to the URL-structure, some are bound to duplicate(as they don't include unique codes)
# Removing duplicates can be done by sorting variable = '.id'
IN.Bribe.df = IN.Bribe.df[!duplicated(IN.Bribe.df$.id), ]

```

We end up with a dataframe with this composition:

```{r import large dataframe, echo = TRUE}


```

It's however obvious that this dataframe need further manipulation/cleaning:

```{r, echo=FALSE}
IN.Bribe.df$post.views = gsub("\\views.*$", "", IN.Bribe.df$post.views)                     # Seperating numeric from views
IN.Bribe.df$city = word(IN.Bribe.df$post.location, +1)                                      # Seperating region and city into two distinct variables
IN.Bribe.df$city = gsub("\\,", "", IN.Bribe.df$city)                                        # ...
IN.Bribe.df$region = word(IN.Bribe.df$post.location, -1)                                    # ...
IN.Bribe.df$bribe.paid = as.numeric(gsub("[^\\d]+", "", IN.Bribe.df$post.title, perl=TRUE)) # Extracting numeric value of bribe from title using PERL-type regular expression

# 3.2) Deleting obsolete variables/columns
IN.Bribe2.df = subset(IN.Bribe.df, , -c(.id, post.title))
IN.Bribe3.df = data.frame(lapply(IN.Bribe2.df, as.character), stringsAsFactors=FALSE)
IN.Bribe3.df$post.date = gsub("\\,", "", IN.Bribe3.df$post.date)
IN.Bribe3.df$month = word(IN.Bribe3.df$post.date)
IN.Bribe3.df$day = word(IN.Bribe3.df$post.date, -2)
IN.Bribe3.df$year = word(IN.Bribe3.df$post.date, -1)

IN.Bribe3.df$post.url = NULL
IN.Bribe3.df$post.location = NULL
IN.Bribe3.df$post.date = NULL

```
```{r, echo=TRUE}
head(IN.Bribe3.df, 5)
```

